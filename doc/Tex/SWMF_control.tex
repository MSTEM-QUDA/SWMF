\section{Execution and Coupling Control}

The control module of SWMF controls the execution and coupling of
components. The control module is controlled by the user through the
component layout file LAYOUT.in and the input parameter file PARAM.in.
Defining the most efficient layout, execution and coupling control
is not an obvious task. In the current version of SWMF the processor
layout of the components is static. This restriction is somewhat
mitigated by the possibility of restart, which allows to change
the processor layout from one run to another.

\subsection{Processor Layout: LAYOUT.in}

Within one run the layout is determined by the \#COMPONENTMAP
command in the LAYOUT.in file. The exact syntax of this file
is documented in the reference manual of the CON\_world class.
Here we provide several examples which will help to develop
a sense of using optimal layouts.

First of all we have to define the processor rank:
it is a number ranging from 0 to $N-1$, where
$N$ is the total number of processors in the run. 
A component can run on a subset of the processors,
which is defined by the rank of the first (root) processor,
the rank of the last processor, and the stride. For
example if the root processor has rank 4, the last processor
has rank 8, and the stride is 2 than the component will
run on 3 processors with ranks 4, 6 and 8.

\subsubsection{One component}

In the simplest case a single component, say the Global
Magnetosphere (GM) is running. The LAYOUT.in file should be
the following
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
GM    0     999    1
#END
\end{verbatim}
If the SWMF is run on less than 999 processors (which is likely),
the rank of the last processor will be reduced to the maximum
rank of the processors, which is $N-1$ if the SWMF is running on $N$
processors.

\subsubsection{One serial and one parallel component}

When two components are used, their layouts may or may not overlap.
An example for overlapping the layouts of the GM and the
Inner Magnetosphere (IM) components is
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
GM    0     999    1
#END
\end{verbatim}
When the component layouts overlap, the two components can run
sequentially only. Since IM is using a single processor only
(because it is not a parallel code), all the other processors 
will be idling while IM is running. This can be rather inefficient,
especially if the CPU time required by IM is not negligible.
A more efficient execution can be achieved with a non-overlapping layout:
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
GM    1     999    1
#END
\end{verbatim}
Note that this layout file will work for any number 
of processors from 2 and up.

\subsubsection{Two parallel components with different speeds}

It is not always possible, or even efficient to use non-overlapping
layouts. For example both the SC and IH components require a lot of memory,
but the IH component runs much faster (say 100 times faster) 
in terms of real time than the SC component (this is due to the 
larger cells and smaller wave speeds in IH).
If we tried to use concurrent execution on 101 processors,
SC should run on 100 and IH on 1 processors to get good load balancing.
However the IH component needs much more memory than available
on a single processor. It is therefore not possible to use a non-overlapping
layout for SC and IH on a reasonable number of processors.

Fortunately both the Solar Corona (SC) and Inner Heliosphere (IH)
components are modeled by \BATSRUS, which is a highly parallel code
with good scaling. The following layout can be optimal:
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IH    0     999    1
SC    0     999    1
#END
\end{verbatim}
Although IH and SC will execute sequentially, they both
use all the available CPU-s, so no CPU is left waiting for the others.

\subsubsection{Two parallel components with similar speeds}

If two parallel components need about the same CPU time/real time
on the same number of processors, the optimal layout can be
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
GM    0     999    2
SC    1     999    2
#END
\end{verbatim}
Here GM is running on the processors with even rank,
while SC is running on the processors with odd ranks.
By using the processor stride, this layout works on
an arbitrary even number of processors.

When more serial and parallel codes are executing together,
finding the optimal layout may not be trivial. 
It may take some experimentation to see which component
is running slower or faster, how much time is spent
on coupling two components, etc. It may be a good idea
to test the components separately or in smaller groups
to see how fast they can execute.

\subsubsection{A complex example with four components}

Here is an example with 4 components: the Ionospheric
Electrodynamics (IE) component can run on 2 processors and 
tt runs about 3 times faster than real time.
The serial Inner Magnetosphere (IM) component runs even faster,
on the other hand the coupling of GM and IM is rather
computationally expensive. The Upper Atmosphere (UA) component
can run on up to 32 processors, and it runs twice as fast
as real time. The Global Magnetosphere model (GM) needs
at least 32 processor to run faster than real time.
If we have a lot of CPU-s, we may simply create a non-overlapping
layout. Since GM has no restriction on the number of processors,
it can be the last componentn in the map
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
IE    0       1    1
UA    2      33    1
GM   34     999    1
#END
\end{verbatim}
This layout will be optimal in terms of speed for a large 
(more than 100) number of PE-s, and actually the maximum
speed is going to be limited by the components which do
not scale. On a more modest number of PE-s one can try
to overlap UA and GM:
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
IE    1       2    1
UA    0      31    1
GM    3     999    1
#END
\end{verbatim}

\subsection{Steady State vs. Time Accurate Execution}

The SWMF can run both in time accurate (default) and
steady state mode. This sounds surprising first, 
since many of the components can run in time accurate 
mode only. However, the SWMF can improve the convergence
towards a steady state by allowing the different components
to run at different speeds in terms of the physical time.
In \BATSRUS the same idea is used on a much smaller scale:
local time stepping allows each cell to converge towards
steady state as fast as possible, limited only by the local
numerical stability limit.

When SWMF runs in steady state mode, the SWMF time is not
advanced. The components may or may not advance their own
internal times. The execution is controlled by the 
step number {\tt nStep}, which goes from its initial value 
to the final step allowed by the MaxIteration parameter
of the \#STOP command. The components are called at
the frequency defined by the \#CYCLE command. For example
\begin{verbatim}
#CYCLE
IM          NameComp
2           DnRun
#END
\end{verbatim}
means that IM runs in every second time step of the SWMF.
By defining the DnRun parameter for all the components,
an arbitrary relative calling frequency can be obtained,
which can optimize the global convergence rate to steady state.
The default frequency is DnRun=1, i.e. the component is
run in every SWMF time step. 

The relative frequency can be important for numerical
stability too. When GM and IM are to be relaxed
to a steady state, the GM/BATSRUS code is running in 
local time stepping mode, while IM/RCM runs in time 
accurate mode internally. Since GM and IM are coupled
both ways, an instabiliy can occur if both GM and IM
are run every time step, because the GM physical time
step is very small, and the MHD solution cannot relax
while being continuously pushed by the IM coupling.
This unphysical instability can be avoided by calling the
IM component less frequently.

In time accurate mode the components advance in time at
approximately the same rate. The component times are
only synchronized when necessary, i.e. when they
are coupled, when restart files are written, or 
at the end of session and execution. Since the time
steps (in terms of physical and/or CPU time) of the components can be 
vastly different, this minimal synchronization provides the 
most possibilities for efficient concurrent execution.

By default the component time steps are limited by the
time of couplings. This means that if GM can take 4 second
times stepe, and it is coupled with IE every 5 seconds,
then every second GM time step will be truncated to 1 second.
There are two ways to avoid this. One is to choose the
coupling frequencies to be round multiples of the time steps
of the two components involved. This works well if both components
have fixed time steps and/or much smaller time steps than the 
couipling frequency.

In certain cases the efficiency can be improved with the
\#COUPLETIME command, which can allow a component to 
step through the coupling time. For example
\begin{verbatim}
#CYCLE
GM             NameComp
F              DoCoupleOnTime
\end{verbatim}
will alow the GM component to use 4 second time steps even
if is coupled at every 5 seconds. Of course this will
make the data transferred during the coupling be 
first order accurate in time.

\subsection{Coupling order}

The default coupling order is usually optimal for accuracy
and consistency, but it may not be optimal for speed.
In particular, the IE/Ridley\_serial component solves
a Poisson type equation for the data received from the 
other components (GM and UA). For sake of accuracy
IE always uses the latest data received from the other
components. If GM, UA and IE are coupled
in the default order
\begin{verbatim}
#COUPLEORDER
4
GM IE
UA IE
IE UA
IE GM
\end{verbatim}
and the to-IE and from-IE coupling times coincide, e.g.
\begin{verbatim}
#COUPLE2
GM            NameComp1
IE            NameComp2
10.0          DtCouple
-1            DnCouple

#COUPLE2
UA            NameComp1
IE            NameComp2
10.0          DtCouple
-1            DnCouple
\end{verbatim}
then GM and UA wil have to wait until IE solves
the Poisson equation, because IE receives new data
and it is required to produce results immediately.
With the reversed coupling order
\begin{verbatim}
#COUPLEORDER
4
IE UA
IE GM
GM IE
UA IE
\end{verbatim}
IE will provide the solution from the previously received data,
and it will have time to work on the new data while GM and UA
are working on their time steps. The reversed coupling order
allows the concurrent execution of IE with other components.
The temporal accuracy, on the other hand, will be first order.

An alternative way to achieve concurrent execution is to
stagger the coupling times. For example the
\begin{verbatim}
#COUPLE2SHIFT
GM                 NameComp1
IE                 NameComp2
-1                 DnCouple
10.0               DtCouple
-1                 nNext12
0.0                tNext12
-1                 nNext21
5.0                tNext21
\end{verbatim}
will schedule a GM to IE coupling at 0, 10, 20, 30, \ldots seconds,
and the IE to GM coupling at 5, 15, 25, \ldots seconds.
