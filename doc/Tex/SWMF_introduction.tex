%\documentclass[a4paper,11pt]{article}
%\author{\bf Center for Space Environment Modeling, The University of Michigan}
%\title{\bf \Large Release Notes for the Milestone 7I and Referance Manual}
%\maketitle



\chapter{Introduction}


This document describes a working prototype of the NASA-funded Space
Weather Modeling Framework (SWMF) delivered to NASA to fulfill the
Milestone 7I requirements. The SWMF was developed to provide flexible
"plug and play" type simulation capabilities serving the Sun-Earth
modeling community.  In its current form the SWMF links together five
models: Global Magnetosphere, Inner Heliosphere, Ionosphere
Electrodynamics, Upper Atmosphere and Inner Magnetosphere. The
framework permits the switching of models of any type.


In its fully developed form, the SWMF will comprise a series of
inter-operating models of physics domains, ranging from the surface of
the Sun to the upper atmosphere of the Earth.  In the future the SWMF
will link the additional physics models: Solar Corona, Solar Energetic
Particles, Eruptive Events, Cometary Environment, Interstellar
Neutrals, Outer Heliosphere, Plasmasphere, Planetary Satellites, Polar
Wind and Radiation Belts.


The SWMF implementation is based on the component technology and
Object-Oriented Programming emulated in Fortran 90.  The SWMF parallel
communications are based on the MPI standard.  In its current
implementation the SWMF creates a single executable.


\section{Acknowledgments}

The SWMF was developed at the Center for Space Environment Modeling
(CSEM) of the University of Michigan under the NASA Earth Science
Technology Office (ESTO) Computational Technologies (CT) Project (NASA
CAN NCC5-614). The project is entitled as ``A High-Performance
Adaptive Simulation Framework for Space-Weather Modeling (SWMF)''.
The Project Director is Professor Tamas Gombosi, and the Co-Principal
Investigators are Professors Quentin Stout and Kenneth Powell.

The architectural design of the SWMF was developed by David Chesney,
Darren DeZeeuw, Tamas Gombosi, Kenneth Hansen, Kevin Kane, Ward (Chip)
Manchester, Robert Oehmke, Kenneth Powell, Aaron Ridley, Ilia Roussev,
Quentin Stout, Igor Sokolov, G\'abor T\'oth and Ovsei Volberg.

{\bf The final architectural design of this prototype} was done by
Ovsei Volberg and G\'abor T\'oth.

{\bf The core code development} for this release was done by G\'abor
T\'oth, Ovsei Volberg, Igor Sokolov and Aaron Ridley.

{\bf The session and time management support} was designed and
developed by G\'abor T\'oth.

{\bf The SWMF coupling toolkit} was developed by Igor Sokolov.

This release includes components for five physics domains: Global
Magnetosphere (GM), Inner Heliosphere (IH), Ionosphere Electrodynamics
(IE), Upper Atmosphere(UA) and Inner Magnetosphere (IM).  The physics
model development for these components was done by the following
research groups:

\begin{itemize}
\item
The Global Magnetosphere (GM) and Inner Heliosphere (IH) components
are based on \BATSRUS\ MHD code developed at CSEM. \BATSRUS\ is a
3-dimensional block-adaptive cartesian code which is highly parallel.

\item
The Inner Magnetosphere (IM) component is the Rice Convection Model
(RCM) developed Rice University.  This code is 2-dimensional and
serial.

\item
The Ionospheric Electrodynamics (IE) component is a 2-processor,
2-dimensional spherical electric potential solver developed at CSEM
(termed the ``Ridley Ionosphere''.  In addition, there are a number of
statistical models included with the distribution, which are described
further in the document.

\item
The Upper Atmosphere (UA) component is the Global Ionosphere -
Thermosphere Model (GITM).  This model is a 3-dimensional spherical
model developed at CSEM.  It is fully parallel.

\end{itemize}

Some additional pieces of code complementary to the core development
of this SWMF prototype were developed in parallel.  Kevin Kane
performed the preliminary design of the Graphical User Interface for
the SWMF.  Robert Oehmke developed the spherical Adaptive Blocks (AB)
library, which is used by GITM.

Darren DeZeeuw, Kenneth Hansen, Ward (Chip) Manchester, Igor Sokolov,
G\'abor T\'oth and Ovsei Volberg did all the testing.


\section{The SWMF in a Few Paragraphs}


The SWMF is a structured collection of software building blocks that
can be used or customized to develop Sun-Earth system modeling
components, and to assemble them into applications. The SWMF consist
of utilities and data structures for coupling model components. The
SWMF contains a Control Module (CON), which is responsible for
component registration, processor layout for each component and
coupling schedules.  It controls initialization and execution of the
components. A component is adapted from user-supplied physics codes,
(for example \BATSRUS\ or RCM), by adding two relatively small units
of code:

\begin{itemize}
\item A wrapper, which provides the control functions, and
\item A coupling interface to perform the data exchange with other
components.
\end{itemize}

Both the wrapper and coupling interface are constructed from the
building blocks provided by the framework.

From component software technology perspective both the wrapper and
coupling interface are component interfaces: the wrapper is an
interface with CON, and the coupling interface is an interface with
another component. In this release the wrappers and coupling
interfaces for all five physics components are included. A physics
model code and its wrapper, which comprise a component, share the
communication group.  The coupling interface uses the union
communicator of the two components that it links together.

A SWMF component is compiled into a separate library that resides in
the directory {\tt lib}, which is created as part of the installation
process described later in this document.  Currently the component
libraries are static libraries. The executable image is created in the
directory {\tt bin}, which is created during the compilation.  If a
user does not want to build some particular component, this component
should be substituted by an empty version of the component.

An important feature of the SWMF is the component registration.  A
component to be included in the run should be registered by the
framework.  Currently entering the line for the component in the input
file called {\tt LAYOUT.in} does the registration.  Thus the SWMF
performs the run-time registration of components.

The framework controls the initialization, execution, coupling and
finalization of components.  The execution is done in sessions. In
each session the parameters of the framework and the components can be
changed.  The parameters are read from the {\tt PARAM.in} file, which
may contain further included parameter files.  These parameters are
read and broadcast by CON and the component specific parameters are
sent to the components. The structure of the parameter file will be
described in detail.

The sessions can be executed in 3 different different ways: (1) The
sequential execution model of the session is based on the \BATSRUS\
code, and it is backward compatible with it. In this model the
components are synchronized at every time step, so typically only one
component is executing (possibly on many processors) at any given
time.  In the parallel execution model, the components communicate
only when necessary.  This is possible, because the coupling times are
known in advance.  The components advance to the time of coupling and
only the processors involved in the coupling need to communicate with
each other.  (2) The parallel execution model requires that there is
no overlap in the layout of the components, i.e. there is exactly one
component running on every processing element.  (3) The general
execution model behaves the same way as the parallel model when there
is no overlap, but it also allows more than one component on the same
PE. Currently the parallel and general models are developed for time
accurate runs only. The generalization for steady state simulations
will be done in the final implementation.

The coupling of the components is realized either with plain MPI
calls, or via the SWMF coupling toolkit, which can couple components
based on the following types of parallel distributed grids:
\begin{itemize}
\item 3-D Block adaptive (AMR) parallel grid
\item 2-D Spherical grid
\item Logically Cartesian uniform grid
\item Logically Cartesian non-uniform grid 
\end{itemize}
The SWMF coupling toolkit performs an efficient N to M parallel
coupling based on a router. The router is calculated in advance using
the domain decomposition and grid description obtained from the
components.  The router is updated only when the domain decompositions
or the grids of the components change, or when the mapping geometry
changes.  The coupling toolkit takes care of linear interpolation in
space based on the grid descriptor.  Temporal interpolation is not
supported by the current implementation.

The framework has been tested on the SGI Origin 2000 and Compaq ES45
machines, and on Linux Beowulf clusters with the NAG F95 compiler.

%-----------------------------------------------------------------------
% Chapter 2
%-----------------------------------------------------------------------

\chapter{Quick Start}

\section{A Brief Description of the SWMF Distribution}

The distribution in the form of the compressed tar image
SWMF\_7I.tar.gz includes the directory structure that comprises the
SWMF and test directories:
\begin{itemize}
\item {\tt Common} - the directory of the framework's main building blocks
\item {\tt srcCON} - the source directory for the Control Module
\item {\tt srcTIMING} - the time profiling source directory
\item {\tt srcNOMPI} - the source directory for an empty MPI library
\item {\tt GM, IH, IE, IM, UA} - the directories of the five
components included in the release
\item {\tt Doc} - the documentation directory
\item {\tt Scripts, Param} - the directories needed to create the run directory
\item {\tt Tests} - the directory of test files and all data for the
                    test result verification
\end{itemize}
The high level directory contains the high-level Makefile, the installation 
turning script swmf.env and the PERL scripts for testing: TestSuite.pl, 
TestSWMF.pl, TestCompare.pl.
The PERL scripts are described in more details in the Reference Manual part of 
this document. 

\section{Installing the Code}

The first step in installing the SWMF is untarring the distribution.
If the tar program knows about the -z flag, you can open the gzipped
tar files with a single UNIX command:
\begin{verbatim}
       tar xzf SOMETARFILE.tgz
\end{verbatim}
If the tar program does not recognize the -z flag, two steps are needed:
\begin{verbatim}
       gunzip SOMETARFILE.tgz
       tar xf SOMETARFILE.tar
\end{verbatim}
In the following descriptions the shorter form is shown, but you may
need to use the two step procedure on certain platforms.

Untar the distribution using the command:
\begin{verbatim}
tar xzf SWMF_7I_Release.tgz
\end{verbatim}

Change directories into the distribution:
\begin{verbatim}
cd SWMF_7I_Release
\end{verbatim}

The SWMF needs to know what architecture you are running the code on.
It can figure this out all by itself, but you have to run the command:
\begin{verbatim}
./swmf.env
\end{verbatim}
in the main directory. This creates {\tt Common/Makefile.COMP} with
the correct absolute path to the base directory, which is needed by
the Makefile-s.  The {\tt swmf.env} script only needs to be run once,
except if the base directory is renamed or moved.

To install the framework, type:
\begin{verbatim}
make install
\end{verbatim} 

\section{Creating Documentation}

The documentation for SWMF can be generated from the distribution by
the command
\begin{verbatim}
make PDF
\end{verbatim}
which creates documentation files in the Adobe PDF format.  The PDF
files are placed into the {\tt Doc} library.  An on-line version can
be created by
\begin{verbatim}
make HTML
\end{verbatim}
The top level HTML file is in {\tt Doc/HTML/index.html}, this the file
to point at with the browser.  To clean the intermediate files type
\begin{verbatim}
cd Doc/Tex
make clean
\end{verbatim}
To remove all the documentation type
\begin{verbatim}
cd Doc/Tex
make distclean
\end{verbatim}

\section{Building and Running an Executable}

Manually edit the file Common/Makefile.COMP to select physics
components for your future runs. The components that are not needed
for the current run should be substituted by their "empty" version.
It is OK to leave this file alone to start with.  The default is to
include all models, which will take up more memory, but will be the
most general.

To build the executable, type:
\begin{verbatim}
make
\end{verbatim} 
Depending on the machine that you are compiling on, this will take
10-30 minutes.  In addition, you may want make the post processing
codes also:
\begin{verbatim}
make PTEC
make PIDL
\end{verbatim} 
These two commands will create the codes {\tt PostTEC.exe}, for post
processing Tecplot files, and {\tt PostIDL.exe} for post processing
IDL files.

The executable should be run in a sub-directory, since a large number
of files are created in each run.  To create this directory use the
command:
\begin{verbatim}
make rundir
\end{verbatim} 
This command creates a directory called {\tt run}.  You can either
leave this directory as named, or {\tt mv} it to a different name.  It
is best to leave it in the same {\tt SWMF\_7I\_Release} directory, since
keeping track of the code version associated with each run is quite
important.  The {\tt run} directory will contain links to the codes
which were created in the previous step as well as subdirectories
where input and output of the different components will reside.

Here we assume that the {\tt run} directory is still called {\tt
run}:
\begin{verbatim}
cd run
\end{verbatim} 
Edit the processor map input file LAYOUT.in to define the processor
layout for the components involved in the future run.  Edit or create
the input parameter file PARAM.in.  The default LAYOUT.in and PARAM.in
files in the run directory are suitable to perform the ``Start'' test
on 16 PE-s.

An example processor map file LAYOUT.in to run the executable with
all five components on 16 processors is:
\begin{verbatim}
#COMPONENTMAP
GM    0    4    1
IE    5    6    1
IH    7   10    1
IM   11   11    1
UA   12   15    1
#END
\end{verbatim}
The file syntax is simple. It must start with the directive
\#COMPONENTMAP and end with another directive \#END. Each line between
these directives specifies the label for component, i.e. IE, GM and
etc., its first and last processor, all relatively to the world
communicator, and the stride. Thus GM will run on 5 processors from 0
to 4, and IM will run on only 1 processor, the processor 11.  If
stride is not equal to 1, the processors for the component will not be
neighboring processors.

Run the executable through submitting a batch job or, if it's possible 
on your computer, run the code interactively.  For example:
\begin{verbatim}
mpirun -np 16 SWMF.exe
\end{verbatim}

To recompile the executable with different compiler settings you have
to use the command:
\begin{verbatim}
make clean
\end{verbatim} 

To discard the configuration and environment settings completely,
type:
\begin{verbatim}
make distclean
\end{verbatim} 

\section{Executing the Test Runs}

We provide a number of test runs for this distribution of the code.
The tests are in the Tests directory for the released code, otherwise
they are in the SMWF\_7I\_TEST CVS repository.  The following tests
are available:
\begin{itemize}
\item {\tt StartTst.tgz}\\
      Steady state run for IH, GM and IE components.
      The run directory is created with the same files as in
      this test.

\item {\tt RestartTst.tgz}\\
      Time accurate run for IH-GM-IM-IE-UA component
      The restart files are for little endian platforms.

\item {RestartTstSGI.tgz}\\
      Same as {\tt RestartTst.tgz} 
      but for big endian platforms such as the SGI.
\end{itemize}
On 16 CPU-s of a Linux cluster (AMD Athlon 1900+, using a 100Mbs
network) the StartTst required 2872 seconds to complete, while the
restart test required 252 seconds with the old time looping, and 182
seconds with the new general time looping (set up time is not
included).

Reference results are also  provided:
\begin{itemize}
\item {\tt StartResult.tgz}\\
       Results for the steady state run with IH, GM and IE

\item {\tt RestartResult.tgz}\\
      Results for the restarted run using the default
       LAYOUT.in and PARAM.in files.

\item {\tt RestartResultGeneral.tgz}\\
      Results for the restarted run using the 
      LAYOUT.in.general and PARAM.in.general files.
\end{itemize}
Only the ASCII output files are included for sake of easy comparison.
Results may vary slightly from platform to platform due to round off errors.

\subsubsection{Running the short restart test}

To run this test you need to install and compile SWMF as described
above and create the run directory. After this you can install the
test into the run directory.  For example on a Linux or TrueUnix
(OSF1) machine the short restart test can be installed with the
following commands:
\begin{verbatim}
	cd run
	tar xzf ../Tests/RestartTst.tgz
\end{verbatim}
On a big endian platform use the {\tt RestartTstSGI.tgz} file instead.

Now you can run the code interactively or submit a job.  The default
layout assumes 16 PE-s. For example an interactive execution could be
like this:
\begin{verbatim}
	mpirun -np 16 SWMF.exe
\end{verbatim}
After the run finished, you may post process the IDL plot files (the
{\tt .idl} and {\tt .h} files produced by GM and IH can be processed
into {\tt .out} files):
\begin{verbatim}
	cd run/GM
	./pIDL
	cd ../IH
	./pIDL
\end{verbatim}
The {\tt .out} files can be visualized with IDL using the scripts in
the Idl directory.  You can also compare the ASCII output files with
the ones given in the reference solutions.

\subsubsection{Test with the general time looping model}

You may modify the LAYOUT.in file as well as use the LAYOUT.in.general
and PARAM.in.general files which test the "general" execution model
(parallel execution, but overlap of components is also allowed). Use
the following commands in the run directory
\begin{verbatim}
	mv LAYOUT.in LAYOUT.in.default
	cp LAYOUT.in.general LAYOUT.in
	mv PARAM.in  PARAM.in.default
	cp PARAM.in.general PARAM.in 
\end{verbatim}
When editing the LAYOUT.in file, remember that the GM and IH
components cannot overlap if they share the same source code!

\subsubsection{Test with the renamed IH code}

The restriction on the overlapping of the IH and GM components
can be eliminated by the use of the renamed IH source code. 
First of all edit {\tt Common/Makefile.COMP} and
select UofM as the {\tt IH\_VERSION}:
\begin{verbatim}
#IH_VERSION = UofM_share
IH_VERSION = UofM
#IH_VERSION = Empty
\end{verbatim}
Now create the renamed source code by typing
\begin{verbatim}
	make IHUOFM
\end{verbatim}
in the main directory. This will take 2-5 minutes. Once the renaming
is done, reduce the number of the blocks for GM and IH by editing
GM/UofM/src/ModSize.f90 and IH/UofM/src/ModSize.f90 to contain
something like this:
\begin{verbatim}
  !\
  ! Block parameters.
  !/
  ! Maximum number of blocks per processor
  integer, parameter :: nBLK=200

  integer, parameter :: MaxBlock = nBLK

  ! Maximum number of implicit blocks                   !^CFG IF IMPLICIT
  integer, parameter :: MaxImplBLK = min(MaxBlock, 1)   !^CFG IF IMPLICIT
\end{verbatim}
The test requires 520 blocks for GM and 456 blocks for IH on the PE-s
assigned to the components. So with at least 3 PE-s for GM and IH each,
the above numbers are sufficient. This test does not require the implicit
scheme, therefore the number of blocks with implicit time stepping can
be minimized. You can specify different number of blocks for GM and IH. 

Recompile the code:
\begin{verbatim}
	make
\end{verbatim}
Now it is allowed to overlap GM and IH. The renamed IH code has the 
version name IH\_BATSRUS, which is printed to the standard output at the
beginning of the execution.


%\end{document}
