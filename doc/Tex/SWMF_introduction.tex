%\documentclass[a4paper,11pt]{article}
%\author{\bf Center for Space Environment Modeling, The University of Michigan}
%\title{\bf \Large Release Notes for the Milestone 7I and Reference Manual}
%\maketitle



\chapter{Introduction}

This document describes a working prototype of the NASA-funded Space
Weather Modeling Framework (SWMF) delivered to NASA to fulfill the
Milestone 11K requirements. The SWMF was developed to provide flexible
``plug and play" type simulation capabilities serving the Sun-Earth
modeling community.  In its current form the SWMF links together eight
models from the surface of the Sun to the upper atmosphere of the Earth: 
\begin{enumerate}
\item GM -- Global Magnetosphere 
\item IE -- Ionosphere Electrodynamics
\item IH -- Inner Heliosphere
\item SP -- Solar Energetic Particles 
\item IM -- Inner Magnetosphere
\item LA -- Lower Atmosphere (under development)
\item PW -- Polar Wind
\item PS -- Plasma Sphere (under development)
\item RB -- Radiation Belts
\item SC -- Solar Corona which includes the Eruptive Event Generator,
\item UA -- Upper Atmosphere
\end{enumerate}
The SWMF may be extended in the future to include even more 
physics domains: Cometary Environment, Interstellar
Neutrals, Outer Heliosphere, Planetary Satellites, and Radiation Transfer.

The SWMF implementation is based on the component technology and
Object-Oriented Programming emulated in Fortran 90.  The SWMF parallel
communications are based on the MPI standard.  In its current
implementation the SWMF creates a single executable. Note, however, that the
models in the SWMF can still be compiled into stand-alone executables.
This means that the models preserve their individuality while being
compatible with the SWMF.

\section{Acknowledgments}

The SWMF was developed at the Center for Space Environment Modeling
(CSEM) of the University of Michigan under the NASA Earth Science
Technology Office (ESTO) Computational Technologies (CT) Project (NASA
CAN NCC5-614). The project is entitled as ``A High-Performance
Adaptive Simulation Framework for Space-Weather Modeling (SWMF)''.
The Project Director is Professor Tamas Gombosi, and the Co-Principal
Investigators are Professors Quentin Stout and Kenneth Powell.

The SWMF and many of the physics components were developed at CSEM
by the following individuals (in alphabetical order):
David Chesney, Yue Deng,
Darren DeZeeuw, Tamas Gombosi, Kenneth Hansen, Kevin Kane, Ward (Chip)
Manchester, Robert Oehmke, Kenneth Powell, Aaron Ridley, Ilia Roussev,
Quentin Stout, Igor Sokolov, G\'abor T\'oth and Ovsei Volberg.

The core design and code development was done by G\'abor
T\'oth, Igor Sokolov and Ovsei Volberg:
\begin{itemize}
\item Component registration and layout was designed and implemented by 
      Ovsei Volberg and G\'abor T\'oth.
\item The session and time management support as well as the various
      configuration scripts and the parameter editor GUI were designed and
      implemented by G\'abor T\'oth.
\item The SWMF coupling toolkit was developed by Igor Sokolov.
\item The SWMF GUI was designed and implemented by Darren De Zeeuw.
\end{itemize}
The physics models were developed by the following research groups:
\begin{itemize}
\item
The Solar Corona (SC), Inner Heliosphere (IH) and the Global Magnetosphere 
(GM) components are based on \BATSRUS\ MHD code developed at CSEM. 
\BATSRUS\ is a 3-dimensional block-adaptive Cartesian code which is 
highly parallel.

\item
The Solar Energetic Particle (SP) component is the
K\'ota's SEP model which was developed at the University of Arizona.
It solves the equations for the advection and acceleration of
energetic particles along a magnetic field line in a 3D phase space.

\item
The Inner Magnetosphere (IM) component is the Rice Convection Model
(RCM) developed Dick Wolf, Stan Sazykin and others at Rice University. 
There are two versions in the SWMF: RCM and RCM2 (with oxygen and loss). 
The RCM code is 2-dimensional in space 
(plus one dimension for energy) and serial.

\item
The Polar Wind (PW) component is the Polar Wind Outflow Model (PWOM)
developed by Alex Glocer, Gabor Toth and Tamas Gombosi
at the University of Michigan.  This code solves the
multifluid equations along multiple field lines and it is fully parallel.

\item
The Radiation Belt Environment (RBE) model is developed by Meiching Fok
at NASA Goddard. It is a spatially 2-dimensional code with 
extra two dimensions for pitch angle and energy. The RBE is serial.

\item
The Ionospheric Electrodynamics (IE) component is a 2-processor,
2-dimensional spherical electric potential solver developed at CSEM
(termed the ``Ridley Ionosphere'').  

\item
There are two versions of the Upper Atmosphere (UA) component:
the Global Ionosphere - Thermosphere Model (GITM) and its newer
version GITM2. Both versions are 3-dimensional spherical
models developed at CSEM by Aaron Ridley, Yue Deng, and Gabor Toth.  
The GITM codes are fully parallel.

\end{itemize}
The transformation of physics models into physics components,
the coupling of components to the SWMF and each other and
all the testing were done at CSEM.

\section{The SWMF in a Few Paragraphs}

The SWMF is a structured collection of software building blocks that
can be used or customized to develop Sun-Earth system modeling
components, and to assemble them into applications. The SWMF consists
of utilities and data structures for coupling model components. The
SWMF contains a Control Module (CON), which is responsible for
component registration, processor layout for each component and
coupling schedules.  It controls initialization and execution of the
components. A component is adapted from user-supplied physics codes,
(for example \BATSRUS\ or RCM), by adding two relatively small units
of code:
\begin{itemize}
\item A wrapper, which provides the control functions, and
\item A coupling interface to perform the data exchange with other
components.
\end{itemize}
Both the wrapper and coupling interface are constructed from the
building blocks provided by the framework. From 
component software technology perspective both the wrapper and
coupling interface are component interfaces: the wrapper is an
interface with CON, and the coupling interface is an interface with
another component. A physics
model code and its wrapper, which comprise a component, share the
communication group.  The coupling interface uses the union
communicator of the two components that it links together.

An SWMF component is compiled into a separate library that resides in
the directory {\tt lib}, which is created as part of the installation
process described later in this document.  Currently the component
libraries are static libraries. The executable image is created in the
directory {\tt bin}, which is created during the compilation.  If a
user does not want to build some particular component, this component
should be substituted by an empty version of the component.

An important feature of the SWMF is the component registration.  A
component to be included in the run should be registered by the
framework.  Currently entering the line for the component in the input
file called {\tt LAYOUT.in} does the registration.  Thus the SWMF
performs the run-time registration of components.

The framework controls the initialization, execution, coupling and
finalization of components.  The execution is done in sessions. In
each session the parameters of the framework and the components can be
changed.  The parameters are read from the {\tt PARAM.in} file, which
may contain further included parameter files.  These parameters are
read and broadcast by CON and the component specific parameters are
sent to the components. The structure of the parameter file will be
described in detail.

If two components reside on different sets of processing elements
(PE-s) they can execute in an efficient concurrent manner.
This is possible, because the coupling times (in terms of the simulation time
or number of iterations) are known in advance.  
The components advance to the time of coupling and
only the processors involved in the coupling need to communicate with
each other. The components are also allowed to share some processing elements.
The execution is sequential for the components with overlapping layouts.
This can be useful when the execution time of the components vary a lot
during the run, or when a component needs a lot of processors 
for memory storage, but it requires little CPU time.
Of course this still allows the individual components to execute in parallel.
For steady state calculations the components are allowed to progress
at different rates towards steady state. Each component can be called
at different frequencies by the control module.

The coupling of the components is realized either with plain MPI
calls, or via the SWMF coupling toolkit, which can couple components
based on the following types of parallel distributed grids:
\begin{itemize}
\item 3-D Block adaptive (AMR) parallel grid
\item 2-D Spherical grid
\item Logically Cartesian uniform grid
\item Logically Cartesian non-uniform grid 
\end{itemize}
The SWMF coupling toolkit performs an efficient N to M parallel
coupling based on a router. The router is calculated in advance using
the domain decomposition and grid description obtained from the
components.  The router is updated only when the domain decompositions
or the grids of the components change, or when the mapping geometry
changes.  The coupling toolkit takes care of linear interpolation in
space based on the grid descriptor.  Temporal interpolation is not
supported by the current implementation.

The framework has been tested on the SGI Origin 3000, SGI Altix and 
Compaq ES45 machines, and on Linux Beowulf clusters with the NAG f95 
compiler. We have also run the framework with reasonable success under
Mac OS Darwin using the XLF and NAG f95 compilers, and under Linux with
the PGF90 compiler.

\section{System Requirements}

In order to install and run the SMWF the following minimum system
requirements apply.

\begin{itemize}
\item The SWMF runs only under the UNIX/Linux operating systems.  This now
  includes Macintosh system 10.x because it is based on BSD UNIX.  The
  SWMF does not run under any Microsoft Windows operating system.
\item A FORTRAN 77 and FORTRAN 90 compiler must be installed.
\item The Perl interpreter must be installed.
\item A version of the Message Passing Interface (MPI) library must be
  installed.
\item You may be able to compile the code and do very small test
runs on 1 or 2 processor machines.  However, to do most physically
meaningful runs the SWMF requires a
parallel processor machine with a minimum of 8 processors and a minimum of 8GB of
memory.
\item Very large runs require many more processors.
\item In order to generate the documentation you must have LaTex installed on
your system.  The PDF generation requires the {\tt dvips} and {\tt ps2pdf}
utilities.  To generate the HTML version you also must install the
{\tt latex2html} package. 

\end{itemize}


In addition to the above requirements, the SWMF output is designed to
be visualized using either IDL or Tecplot.  You may be able to
visualize the output with other packages, but formats and scripts have
been designed for only these two visualization softwares.




%-----------------------------------------------------------------------
% Chapter 2
%-----------------------------------------------------------------------

\chapter{Quick Start}

\section{A Brief Description of the SWMF Distribution}

The distribution in the form of the compressed tar image
includes the SWMF source code.
The top level directory contains the following subdirectories:
\begin{itemize}\itemsep=0pt
\item {\tt CON}     - the directory of the framework's main building blocks
\item {\tt GM}      - Global Magnetosphere component       %^CMP IF GM
\item {\tt IE}      - Ionosphere Electrodynamics Component %^CMP IF IE
\item {\tt IH}      - Inner Heliosphere component          %^CMP IF IH
\item {\tt LA}      - Lower Atmosphere component           %^CMP IF LA
\item {\tt IM}      - Inner Magnetosphere component        %^CMP IF IM
\item {\tt PS}      - Plasma Sphere component              %^CMP IF PS
\item {\tt PW}      - Polar wind component                 %^CMP IF PW
\item {\tt RB}      - Radiation Belt component             %^CMP IF RB
\item {\tt SC}      - Solar Corona component               %^CMP IF SC
\item {\tt SP}      - Solar Energetic Particles component  %^CMP IF SP
\item {\tt UA}      - Upper Atmosphere component           %^CMP IF UA
\item {\tt ESMF}    - the ESMF wrapper for the SWMF
\item {\tt Copyrights} - copyright files
\item {\tt Param}   - description of CON parameters, parameter and layout files
\item {\tt Scripts} - shell and Perl scripts
\item {\tt bin}     - scripts for installation, configuration and testing
\item {\tt doc}     - the documentation directory %^CMP IF DOC
\item {\tt gui}     - the SWMF graphical user interface
\item {\tt output}  - reference test results for the SWMF tests
\item {\tt share}   - shared scripts and source code
\item {\tt util}    - general utilities such as TIMING and NOMPI
\end{itemize}
and the following files
\begin{itemize}\itemsep=0pt
\item {\tt README}        - a short instruction on installation and usage
\item {\tt Makefile}      - the main makefile
\item {\tt Makefile.test} - the makefile containing the tests %#^CMP IF TESTING
\item {\tt Config.pl}     - Perl script for (un)installation and configuration
\end{itemize}

\section{General Hints}

\subsubsection{Getting help with scripts and the Makefile}

Most of the Perl and shell scripts that are distributed with the SWMF
provide help which can be accessed as follows using the {\tt -h} flag.
For example, 
\begin{verbatim}
  Config.pl -h
\end{verbatim}
will provide a detailed listing of the options and capabilities of the
{\tt Config.pl} script.  In addition, you can find all the possible
targets  that can be built by typing
\begin{verbatim}
make help
\end{verbatim}

\subsubsection{Input commands: PARAM.XML}
A very useful set of files to become familiar with are the {\tt PARAM.XML}
files.  Such a file exists for the SWMF itself and for each of the
physics components.  The file for the SWMF is found at
\begin{verbatim}
Param/PARAM.XML
\end{verbatim}
while the files for the physics components are found in the component's
subdirectory.  For example, the file for the GM/BATSRUS component can
be found at
\begin{verbatim}
GM/BATSRUS/PARAM.XML
\end{verbatim}
This file contains a complete list of all input commands for the
component as well as the type, the allowed ranges and default values
for each of the input parameters.
Although the XML format makes the files a little hard to read, they are
extremely useful.  A typical usage is to cut and paste commands out of the
PARAM.XML file into the PARAM.in file for a run. 

An alternative approach is to use the web browser based parameter editor 
to edit the PARAM.in file for the SWMF 
(also for the stand-alone models that have PARAM.XML files).
The editor GUI can be started as
\begin{verbatim}
share/Scripts/ParamEditor.pl
\end{verbatim}
This editor allows constructing PARAM.in files with pull down menus, 
shows the manual for the edited commands, and checks the correctness of
the parameter file and highlights the errors. All this functionality 
is based on the PARAM.XML files.

\subsubsection{Have the working directory in your path}

In order to run executable files in the UNIX environment you must have
the current working directory either your path or in the filename you
want to execute. In UNIX the current working directory is represented
by the period (.).  For example
\begin{verbatim} 
./Config.pl -s
\end{verbatim}
will execute the Config.pl script if it is in your current directory.  
If you add the `.' to your path using for example
\begin{verbatim}
set path = (${path} .)
\end{verbatim}
then you can simply type
\begin{verbatim} 
Config.pl -s
\end{verbatim}
Setting the path is best done in the .cshrc or equivalent Unix shell 
customization file located in the user's home directory.

\section{Installing the Code}

The first step in installing the SWMF is untarring the distribution.
If the tar program knows about the -z flag, you can open the gzipped
tar files with a single UNIX command:
\begin{verbatim}
  tar xzf SOMETARFILE.tgz
\end{verbatim}
If the tar program does not recognize the -z flag, two steps are needed:
\begin{verbatim}
  gunzip SOMETARFILE.tgz
  tar xf SOMETARFILE.tar
\end{verbatim}
In the following descriptions the shorter form is shown, but you may
need to use the two step procedure on certain platforms.

Untar the distribution using the command:
\begin{verbatim}
  tar xzf SWMF.tgz
\end{verbatim}

Change directories into the distribution:
\begin{verbatim}
  cd SWMF
\end{verbatim}

The SWMF needs to know what architecture you are running the code on
and what FORTRAN compiler will be used.  For most platforms and compilers,
it can figure this out all by itself. To install SWMF run the command:
\begin{verbatim}
  Config.pl -install
\end{verbatim}
in the main directory. This creates {\tt Makefile.def} with
the correct absolute path to the base directory and {\tt Makefile.conf}
which contains the operating system and compiler specific part of
the Makefile. If the compiler is not the default one for a given
platform (e.g. not the NAG f95 compiler for a Linux platform) then
the compiler must be specified explicitly with the {\tt -compiler}
flag. If the MPI header file is not the default one, it can be
specified with the {\tt -mpi} flag. For example on a Mac machine
one can select the g95 compiler with the Intel version of the 
MPI library using
\begin{verbatim}
  Config.pl -install -compiler=g95 -mpi=Intel
\end{verbatim}
To uninstall SWMF type
\begin{verbatim}
  Config.pl -uninstall
\end{verbatim}
If the uninstallation fails (this can happen if some makefiles are missing)
do reinstallation with
\begin{verbatim}
  Config.pl -install
\end{verbatim}
and then try uninstalling the code again.
When SWMF is installed, its configuration can be checked with
\begin{verbatim}
  Config.pl -show
\end{verbatim}
To get a list of the available component versions type
\begin{verbatim}
  Config.pl -l
\end{verbatim}
To get a complete description of the {\tt Config.pl}  script type
\begin{verbatim}
  Config.pl -h
\end{verbatim}

\section{Platform specific information}

\subsection{Altix machine columbia at NASA Ames}

On columbia you should load the 10.0.026 version of the Intel Fortran compiler 
with the command
\begin{verbatim}
  module load intel-comp.10.0.026
\end{verbatim}
Selecting the correct compiler version is 
necessary both to compile and to run the code. This can be done
by manually loading the module before compilation and inserting
the module load command into the job script. A simpler and more
reliable solution is to load the module during login. 
Simply add the following into the .cshrc file in the home directory
\begin{verbatim}
   if(`which ifort` != "/opt/intel/comp/10.0.026/bin/ifort") then
      module load intel-comp.10.0.026
   endif
\end{verbatim}
The if statement avoids loading the module repeatedly.

\subsection{Linux cluster nyx at the University of Michigan}

On nyx at you can either use the pgf90 compiler with the OpenMPI library
\begin{verbatim}
   Config.pl -install -compiler=pgf90 -mpi=openmpi
\end{verbatim}
or the NAG compiler with MPICH library
\begin{verbatim}
   Config.pl -install
\end{verbatim}
In the latter case the PGF90 and OpenMPI modules need to be 
unloaded and the NAG compiler and corresponding MPICH library loaded:
\begin{verbatim}
   module unload openmpi/1.0.2-pgi
   module unload pgi/6.1
   module load mpich/1.2.7-nag
\end{verbatim}
This can be automated by adding the following lines to the .cshrc file:
\begin{verbatim}
if ('which mpif90' != ``/home/software/rhel4/mpich/1.2.7-nag/bin/mpif90'') then
   module unload openmpi/1.0.2-pgi
   module unload pgi/6.1
   module load mpich/1.2.7-nag
endif
\end{verbatim}
The if statement avoids loading the module repeatedly.
Note that on nyx the tcsh shell has to be started manually after login
by typing 'tcsh'.

\section{Creating Documentation}

The documentation for SWMF can be generated from the distribution by
the command
\begin{verbatim}
  make PDF
\end{verbatim}
which creates the user manual
\begin{verbatim}
  doc/SWMF.pdf
\end{verbatim}
and several other documents in the Adobe PDF format.  
In order for this to work you must have
LaTex installed on your system (and dvips and ps2pdf).  
An on-line version can be created by
\begin{verbatim}
  make HTML
\end{verbatim}
The HTML version is generated from the LaTex using the command 
{\tt latex2html}.
You will have to install this if it does not already exist on your system.
The top level HTML file is in
\begin{verbatim}
  doc/index.html
\end{verbatim}
to point at with the browser.  This html file lists the different
documentation files and what they contain.  
To clean the intermediate files type
\begin{verbatim}
  cd doc/Tex
  make clean
\end{verbatim}
To remove all the created documentation type
\begin{verbatim}
  cd doc/Tex
  make cleanall
\end{verbatim}

\section{Building and Running an Executable}

At compile time, the user can select which physics components should be
compiled.  
Any component not compiled will not be available for
use at run time.  The physics components can be selected with the {\tt -v} flag
of the Config.pl script. For example typing
\begin{verbatim}
  Config.pl -v=Empty,SC/BATSRUS,IH/BATSRUS,SP/Kota
\end{verbatim}
will select BATSRUS for the SC and IH components and K\'ota's model for
the SP component and the other components are set to Empty versions
that contain empty subroutines for compilation, but cannot be used.
The default configuration includes a working version for all components, 
which takes up more memory, but is the most general.
The only exception is SC, which requires configuration, so the 
default version is Empty for the Solar Corona component.

The grid size of several components can also be set with the {\tt -g}
flag of the {\tt Config.pl} script. For example the 
\begin{verbatim}
  Config.pl -g=GM:8,8,8,400,100
\end{verbatim}
command sets the block size for the GM component to $8\times 8\times 8$ cells, 
the maximum number of blocks per processor to 400, 
and the maximum number of implicit blocks per processor to 100.
The main SWMF Config.pl script actually runs the individual Config.pl
scripts in the component versions. These scripts can be run directly,
For example try
\begin{verbatim}
  cd GM/BATSRUS
  Config.pl -show
\end{verbatim}
Compilation flags, such as the precision and optimization 
level are stored in {\tt Makefile.conf}. This file is created on
installation of the SWMF and has defaults which are appropriate for
your system architecture.  The precision of reals
can be changed to single precision (for example) by typing
\begin{verbatim}
  Config.pl -single
\end{verbatim}
while the compiler flags can be modified with
\begin{verbatim}
  Config.pl -debug -O0
\end{verbatim}
to debug the code with 0 optimization level, and
\begin{verbatim}
  Config.pl -nodebug -O4
\end{verbatim}
to run the code at maximum optimization level and without the debugging flags.

Before compiling SWMF it is always a good idea to check its configuration
with
\begin{verbatim}
  Config.pl -show
\end{verbatim}

To build the executable {\bf bin/SWMF.exe}, type:
\begin{verbatim}
  make
\end{verbatim} 
Depending on the configuration, the compiler settings and the machine 
that you are compiling on, this can take from 2 to up to 30 minutes.  
In addition, you may want to make the post processing
codes (for BATSRUS only) also:
\begin{verbatim}
  make PSPH
  make PIDL
\end{verbatim} 
These two commands will create the codes {\tt bin/PostSPH.exe}, for post
processing spherical Tecplot files, and {\tt bin/PostIDL.exe} 
for post processing IDL files.

The {\tt SWMF.exe} executable should be run in a sub-directory, since a large number
of files are created in each run.  To create this directory use the
command:
\begin{verbatim}
  make rundir
\end{verbatim} 
This command creates a directory called {\tt run}.  You can either
leave this directory as named, or {\tt mv} it to a different name.  It
is best to leave it in the same SWMF directory, since
keeping track of the code version associated with each run is quite
important.  The {\tt run} directory will contain links to the codes
which were created in the previous step as well as subdirectories
where input and output of the different components will reside.

Here we assume that the {\tt run} directory is still called {\tt
run}:
\begin{verbatim}
  cd run
\end{verbatim}
In order to run the SWMF you must have two input files:  LAYOUT.in and
PARAM.in.  The LAYOUT.in file defines the processor
layout for the components involved in the future run.  The PARAM.in
file contains the detailed commands for controlling what you want the
code to do during the run.  The default LAYOUT.in and PARAM.in
files in the run directory are suitable to perform the ``Start'' test
on 16 processors (PE-s). 

An example processor map file LAYOUT.in to run the executable with
five components on 16 processors is:
\begin{verbatim}
#COMPONENTMAP
GM    0    4    1
IE    5    6    1
IH    7   10    1
IM   11   11    1
UA   12   15    1
#END
\end{verbatim}
The file syntax is simple. It must start with the directive
\#COMPONENTMAP and end with another directive \#END. Each line between
these directives specifies the label for component, i.e. IE, GM and
etc., its first and last processor, all relatively to the world
communicator, and the stride. Thus GM will run on 5 processors from 0
to 4, and IM will run on only 1 processor, the processor 11.  If
stride is not equal to 1, the processors for the component will not be
neighboring processors.

It is strongly recommended to check the validity of the {\tt run/PARAM.in} and 
{\tt run/LAYOUT.in} files before running the code. If the
code will be run on 16 processors, type
\begin{verbatim}
Scripts/TestParam.pl -n=16
\end{verbatim}
in the main SWMF directory.
The Perl script reports inconsistencies and errors. 
If no errors are found, the script finishes silently.
Now you are ready to run the executable through submitting a batch job or, 
if it is possible on your computer, run the code interactively.  For
example, to run the SWMF interactively:
\begin{verbatim}
cd run
mpirun -np 16 SWMF.exe
\end{verbatim}
The SWMF provides example job scripts for several architectures and
machines used by the developers. These job scripts are found in 
\begin{verbatim}
CON/Scripts
\end{verbatim}
in the subdirectories named after the operating system. If the name
of the file in the appropriate subdirectory matches the 
name of the machine, the job script is copied into
the {\tt run} directory when it is created.
These job scripts serve as a starting point only, they must
be customized before they can be used for submitting a job.

To recompile the executable with different compiler settings you have
to use the command
\begin{verbatim}
make clean
\end{verbatim}
before recompiling the executables. It is possible to recompile
only a component or just one subdirectory if the {\tt make clean}
command is issued in the appropriate directory.

\section{Post-Processing the Output Files}

Several components produce output files (plot files) that require
some post-processing before they can be visualized. The post-processing
collects data written out by different processors, and it can also
process and transform the data. 

The PostProc.pl script greatly simplifies the post-processing and
it also helps to collect the run results in a well contained directory tree.
The script can also be used to do post-processing while the code is running.
Usually the processed output files are much smaller than the raw output file,
so post-processing during the run can limit the amount of disk space used
by the raw data files. It also avoids the need to wait for a long time 
for the post-processing after the run is done. 

The PostProc.pl script is copied into the run directory and it should
be executed from the run directory.
To demonstrate the use of the script, here are a few simple examples.
After or during a run, you may simply type
\begin{verbatim}
cd run
./PostProc.pl
\end{verbatim}
to post-process the available output files. The series of individual 
IDL plot files can be concatenated into single movie files with
\begin{verbatim}
./PostProc.pl -M
\end{verbatim}
Repeat the post-processing every 360 seconds during the run,
gzip large ASCII files and create IDL movie files
\begin{verbatim}
./PostProc.pl -r=360 -g -M >& PostProc.log &
\end{verbatim}
After the run is finished, create a directory tree with the output
of all the components, the input parameter file and the 'runlog' file
(if present)
\begin{verbatim}
./PostProc.pl -o RESULTS/NewRun
\end{verbatim}
The RESULTS/NewRun directory will contain the PARAM.in file, the
runlog file (the standard output should be piped into that file),
and the output files for each component in a subdirectory named
accordingly (eg. RESULTS/NewRun/GM/). The output directories of
the components (e.g. GM/IO2/) will be empty.

To see all the options of the script type
\begin{verbatim}
./PostProc.pl -h
\end{verbatim}

\section{Restarting a Run}

There are several reasons for restarting a run. A run may fail
due to a run time error, due to hardware failure, due to 
software failure (e.g. the machine crashes) or because the
queue limits are exceeded. In such a case the run can be continued from
the last saved state of SWMF. 

It is also possible that one builds up a complex simulation from multiple 
runs. For example the first run creates a steady state for the SC component.
The second run includes both the SC and IH components and it 
restarts from the results of the first run and creates a steady state
for both components. A third run may restart from this solution and include
the GM component, etc. 

The restart files are saved at the frequency determined in the PARAM.in file.
Normally the restart files are saved into the output restart directories
of the individual components and subsequent saves overwrite the previous ones
(to reduce the required disk space). A restart requires the modification
of the PARAM.in file: one needs to include the restart file for the
control module of SWMF as well as ask for restart by all the components.

The Restart.pl script simplifies the work of the restart in several ways:
\begin{enumerate}
\item The SWMF restart file and the individual output restart 
directories of the components are collected into a single directory tree, 
the {\bf restart tree}.
\item The default input restart file of SWMF and the default 
      input directories of the components can be linked to an existing
      restart tree.
\item The script can run continuously in the background and create
      multiple restart trees while SWMF is running. 
\item The script does extensive checking of the consistency 
      of the restart files.
\end{enumerate}
The Restart.pl script is copied into the run directory and it should
be executed in the run directory. Note that the PARAM.in file is not
modified by the script: it has to be modified with an editor as needed.

To demonstrate the use of the script, here are a few simple examples.
After a successful or failed run which should be continued, simply type
\begin{verbatim}
cd run
./Restart.pl
\end{verbatim}
to create a restart tree from the final output and to link to the tree for the
next run. The default name of the restart tree is based on the simulation time
for time accurate runs, or the time step for non-time accurate runs.
But you can also specify a name explicitly, for example
\begin{verbatim}
./Restart.pl RESTART_SC_steady_state
\end{verbatim}
If you wish to continue the run in another run directory, or on another
machine, transfer the restart tree as a whole into the new run
directory and type
\begin{verbatim}
./Restart.pl -i=RESTART_SC_steady_state
\end{verbatim}
where the {\tt -i} stands for ``input only'', i.e. the script links to
the tree, but it does not attempt to create the restart tree.

To save multiple restart trees repeatedly at an hourly frequency of 
wall clock time while the SWMF is running, type
\begin{verbatim}
./Restart.pl -r=3600 &
\end{verbatim}
To see all the options of the script type
\begin{verbatim}
./Restart.pl -h
\end{verbatim}

\section{What next?}

Hopefully this section has guided you through installing the SWMF and
given you a basic knowledge of how to run it.  However it has probably
also convinced you that the SWMF is quite a complex tool and that there
are many more things for you to learn.  So, what next?

We suggest that you read all of chapter \ref{chapter:basics}, which
outlines the basic features of the SWMF as well as some things you
really must know in order to use the SWMF.  Once you have done this you
are ready to experiment.  Chapter \ref{chapter:examples} gives several 
examples which are intended to make you familiar with the use of the
SWMF.  We suggest that you try them!

%\end{document}
